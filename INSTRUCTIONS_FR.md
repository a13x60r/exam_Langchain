# Examen LangChain : Assistant de Tests Unitaires Python

## Consignes gÃ©nÃ©rales

Lâ€™examen a pour objectif de dÃ©velopper un assistant intelligent capable dâ€™analyser du code Python, de gÃ©nÃ©rer automatiquement des tests unitaires avec pytest, et dâ€™expliquer ces tests de maniÃ¨re pÃ©dagogique.

Pour y parvenir, vous devrez mettre en place une architecture complÃ¨te combinant plusieurs outils : 

- **LangChain** pour gÃ©rer les chaÃ®nes, les prompts, les parsers et la mÃ©moire
- **FastAPI** pour exposer les fonctionnalitÃ©s Ã  travers une API sÃ©curisÃ©e
- **Docker** avec un **Makefile** afin de conteneuriser et dâ€™orchestrer lâ€™ensemble du projet. 
- Une interface utilisateur avec **Streamlit** peut Ãªtre ajoutÃ©e en complÃ©ment, mais elle reste OPTIONELLE.

Pour rÃ©aliser cet examen, un [rÃ©pertoire GitHub](https://github.com/DataScientest/exam_Langchain) vous est mis Ã  disposition. La premiÃ¨re Ã©tape consiste Ã  cloner ce dÃ©pÃ´t sur votre machine afin de disposer de toute la structure de projet attendue (dossiers, fichiers de configuration, Makefile, etc.). 

Ce dÃ©pÃ´t sert de squelette : il vous fournit lâ€™architecture de base que vous devrez complÃ©ter en implÃ©mentant les diffÃ©rents composants (chaÃ®nes LangChain, parsers, mÃ©moire, API, conteneurisation).

```txt
exam_Langchain/                     
â”œâ”€â”€ .env                             # Variables d'environnement (clÃ©s API GROQ + LangSmith)
â”œâ”€â”€ .python-version                  # Version de Python utilisÃ©e (ici 3.13)
â”œâ”€â”€ pyproject.toml                   # Gestion des dÃ©pendances et configuration du projet
â”œâ”€â”€ Makefile                         # Commandes pour build/up/down les conteneurs
â”œâ”€â”€ docker-compose.yml               # Orchestration des services (auth, main, streamlit)
â”œâ”€â”€ README.md                        # Documentation principale du projet
â””â”€â”€ src/                             # Code source du projet
    â”œâ”€â”€ api/                         # Dossier regroupant les services API
    â”‚   â”œâ”€â”€ authentification/        # Service dâ€™authentification 
    â”‚   â”‚   â”œâ”€â”€ Dockerfile.auth      
    â”‚   â”‚   â”œâ”€â”€ requirements.txt     
    â”‚   â”‚   â””â”€â”€ auth.py              # Code FastAPI pour gÃ©rer signup, login, me
    â”‚   â””â”€â”€ assistant/               # Service principal de lâ€™assistant LangChain
    â”‚       â”œâ”€â”€ Dockerfile.main      
    â”‚       â”œâ”€â”€ requirements.txt     
    â”‚       â””â”€â”€ main.py              # Code FastAPI pour analyse/gÃ©nÃ©ration/tests/chat
    â”œâ”€â”€ core/                        # Composants centraux LangChain
    â”‚   â”œâ”€â”€ llm.py                   # Configuration du modÃ¨le de langage (LLM + fallback)
    â”‚   â”œâ”€â”€ chains.py                # DÃ©finition des diffÃ©rentes chaÃ®nes (analyse, test, etc.)
    â”‚   â””â”€â”€ parsers.py               # Parsers Pydantic pour structurer les sorties du LLM
    â”œâ”€â”€ memory/                       
    â”‚   â””â”€â”€ memory.py                # Fonctions pour gÃ©rer lâ€™historique multi-utilisateurs
    â”œâ”€â”€ prompts/                     
    â”‚   â””â”€â”€ prompts.py               # Prompts pour analyse, gÃ©nÃ©ration, explication, chat
    â”œâ”€â”€ Dockerfile.streamlit (optionnel)   # Dockerfile pour lâ€™interface utilisateur Streamlit
    â”œâ”€â”€ requirements.txt (optionnel)       # DÃ©pendances pour lâ€™app Streamlit
    â””â”€â”€ app.py (optionnel)                  # Application Streamlit pour interagir avec lâ€™assistant
```

Lâ€™ensemble des consignes dÃ©crites ci-dessous doit Ãªtre suivi en vous appuyant sur cette structure dÃ©jÃ  prÃ©parÃ©e, que vous enrichirez progressivement pour aboutir Ã  un assistant fonctionnel.

### Le LLM (``src/core/llm.py``)

Le cÅ“ur de lâ€™assistant repose sur le modÃ¨le de langage (LLM), qui est responsable de la gÃ©nÃ©ration et de lâ€™interprÃ©tation des rÃ©ponses. Ce fichier a pour rÃ´le de configurer et dâ€™initialiser le modÃ¨le choisi, ainsi que de prÃ©voir une solution de repli en cas de problÃ¨me.

Lâ€™implÃ©mentation doit inclure :

- **Un modÃ¨le principal** : câ€™est le modÃ¨le par dÃ©faut utilisÃ© pour toutes les requÃªtes (par exemple un modÃ¨le Groq LLaMA 70B).
- **Une rÃ©cupÃ©ration des clÃ©s API**: les identifiants et paramÃ¨tres sensibles doivent Ãªtre stockÃ©s dans le fichier ``.env`` et rÃ©cupÃ©rÃ©s dans le code via des variables dâ€™environnement.

Cette couche dâ€™abstraction permet de sÃ©parer clairement la logique mÃ©tier (chaÃ®nes, prompts, parsers) de la configuration du modÃ¨le. Ainsi, il est facile de changer de fournisseur ou de modÃ¨le sans avoir Ã  modifier lâ€™ensemble du projet.

### Les Prompts (src/prompts/prompts.py)

Les prompts jouent un rÃ´le central dans lâ€™architecture, car ce sont eux qui dÃ©finissent la maniÃ¨re dont le modÃ¨le doit raisonner et formuler ses rÃ©ponses. Ils servent dâ€™instructions claires et contraignantes au LLM pour garantir que les sorties soient exploitables.

Dans cet examen, vous devez **mettre en place diffÃ©rents prompts** correspondant aux fonctionnalitÃ©s attendues de lâ€™assistant :

- **Prompt dâ€™analyse de code** : Demande au LLM dâ€™Ã©valuer un extrait de code Python et de dÃ©terminer sâ€™il est optimal. Le modÃ¨le doit identifier dâ€™Ã©ventuels problÃ¨mes (lisibilitÃ©, performance, bonnes pratiques manquantes) et proposer des amÃ©liorations.
- **Prompt de gÃ©nÃ©ration de tests unitaires** : A partir dâ€™une fonction Python donnÃ©e, lâ€™assistant doit produire un test unitaire en pytest. La consigne doit obliger le modÃ¨le Ã  rÃ©pondre avec un contenu structurÃ©, afin de pouvoir extraire le code du test automatiquement.
- **Prompt dâ€™explication de tests** : Explication pÃ©dagogique et dÃ©taillÃ©e dâ€™un test unitaire. Lâ€™assistant doit se comporter comme un professeur et rendre le test comprÃ©hensible pour un Ã©tudiant ou un dÃ©veloppeur dÃ©butant.
- **Prompt de conversation libre** : Discussion naturelle avec lâ€™utilisateur. Ce prompt doit Ãªtre conÃ§u pour fonctionner avec la mÃ©moire conversationnelle, en intÃ©grant lâ€™historique des Ã©changes afin de donner de la continuitÃ© au dialogue.

Chaque prompt doit Ãªtre construit de faÃ§on Ã  toujours produire une rÃ©ponse en JSON valide, afin de pouvoir Ãªtre interprÃ©tÃ©e par les parsers.

> âš ï¸ **Attention** : veillez Ã  bien intÃ©grer les variables placeholders (**``{input}``**, **``{format_instructions}``**, etc.) pour que le modÃ¨le reÃ§oive les bonnes informations. Pour le chat avec mÃ©moire, lâ€™utilisation de **``MessagesPlaceholder``** est obligatoire afin de transmettre correctement lâ€™historique des conversations au LLM.

### Les Parsers (``src/core/parsers.py``)

Les parsers constituent une Ã©tape essentielle du projet : ils permettent de convertir les rÃ©ponses brutes du modÃ¨le en objets structurÃ©s et exploitables. Comme le LLM renvoie du texte, il est indispensable de transformer ces sorties en formats clairs (par exemple JSON) pour pouvoir les manipuler dans lâ€™API et la mÃ©moire.

Chaque fonctionnalitÃ© de lâ€™assistant est associÃ©e Ã  un parser dÃ©diÃ© :

- **Parser dâ€™analyse de code** : Transformer la rÃ©ponse du modÃ¨le en un objet contenant trois informations clÃ©s : 
    - code optimal ou non
    - une liste des problÃ¨mes dÃ©tectÃ©s
    - une liste des suggestions dâ€™amÃ©lioration 
- **Parser de gÃ©nÃ©ration de tests**: Extraire du texte brut uniquement la partie correspondant au code du test unitaire en pytest, sous forme exploitable et directement exÃ©cutable.
- **Parser dâ€™explication de tests** : Convertir la sortie du modÃ¨le en une explication claire et pÃ©dagogique, sous forme de texte structurÃ©.

Ces parsers doivent Ãªtre construits avec Pydantic, ce qui garantit :

- Une validation stricte du format attendu.
- La sÃ©rialisation facile en dictionnaires (``.dict()``) pour le retour dans les endpoints.
- Une robustesse accrue face aux erreurs de format du modÃ¨le.

### Les ChaÃ®nes (``src/core/chains.py``)

Les chaÃ®nes LangChain constituent le cÅ“ur logique de lâ€™assistant : elles orchestrent le flux dâ€™information entre les prompts, le modÃ¨le de langage et les parsers. Chaque fonctionnalitÃ© repose sur une chaÃ®ne dÃ©diÃ©e, qui dÃ©finit clairement comment le LLM doit Ãªtre sollicitÃ© et comment sa sortie doit Ãªtre exploitÃ©e.

Vous devez mettre en place plusieurs chaÃ®nes :

- **ChaÃ®ne dâ€™analyse de code** : Utilise le prompt dâ€™analyse, envoie la requÃªte au LLM, puis parse la rÃ©ponse pour obtenir un objet structurÃ© contenant lâ€™Ã©valuation (optimalitÃ©, problÃ¨mes, suggestions).
- **ChaÃ®ne de gÃ©nÃ©ration de tests unitaires** : Prend en entrÃ©e une fonction Python et renvoie un test unitaire au format pytest. 
- **ChaÃ®ne dâ€™explication de tests** : Transforme un test Python en une explication claire et pÃ©dagogique destinÃ©e Ã  un utilisateur humain.
- **ChaÃ®ne de chat libre** : Permet une conversation libre. Contrairement aux autres chaÃ®nes, elle ne passe pas par un parser mais doit intÃ©grer la mÃ©moire pour assurer une continuitÃ© dans les Ã©changes.

Chaque chaÃ®ne doit Ãªtre construite de maniÃ¨re simple et modulaire, afin que lâ€™API puisse les invoquer directement sans logique supplÃ©mentaire.

### La MÃ©moire (``src/memory/memory.py``)

La mÃ©moire doit Ãªtre implÃ©mentÃ©e de maniÃ¨re Ã  gÃ©rer plusieurs utilisateurs en parallÃ¨le. Lâ€™idÃ©e est de disposer dâ€™un store global qui associe chaque **session_id** Ã  un historique de type ``InMemoryChatMessageHistory``.

Deux fonctions principales doivent Ãªtre codÃ©es :

- **``get_session_history(session_id)``** : Retourne lâ€™historique de la session pour un utilisateur donnÃ©. Si aucune session nâ€™existe encore pour cet utilisateur, une nouvelle instance dâ€™historique doit Ãªtre crÃ©Ã©e automatiquement.
- **``get_user_history(session_id)``** : Permet de rÃ©cupÃ©rer lâ€™ensemble de lâ€™historique de lâ€™utilisateur sous forme de liste de dictionnaires, avec pour chaque message le rÃ´le (human ou ai) et le contenu.

> âš ï¸ Points importants Ã  respecter :
>
> - Le session_id doit Ãªtre unique par utilisateur (exemple : le nom dâ€™utilisateur renvoyÃ© par le JWT).
> - La mÃ©moire est non persistante : elle sera rÃ©initialisÃ©e si lâ€™application est relancÃ©e.
> - Ce systÃ¨me doit Ãªtre utilisÃ© en particulier dans la chaÃ®ne de chat, avec ``RunnableWithMessageHistory``, pour assurer la continuitÃ© des conversations.

### Les APIs (``src/api/``)

L'examen se repose sur deux APIs distinctes, toutes deux dÃ©veloppÃ©es avec FastAPI et exÃ©cutÃ©es dans des conteneurs sÃ©parÃ©s :

#### Lâ€™API dâ€™authentification (``src/api/authentification/``)

Cette API est dÃ©diÃ©e Ã  la gestion de la sÃ©curitÃ© et des utilisateurs. Elle doit permettre :

- **Lâ€™inscription (signup)** : crÃ©er un nouvel utilisateur et lâ€™enregistrer dans une base (ici simulÃ©e par une structure interne).
- **La connexion (login)** : vÃ©rifier les identifiants permettant dâ€™accÃ©der aux autres services.

Chaque endpoint doit Ãªtre protÃ©gÃ© et renvoyer des erreurs claires en cas de problÃ¨me (utilisateur existant, identifiants incorrects). Le service dispose de son propre Dockerfile et de dÃ©pendances spÃ©cifiques.

#### Lâ€™API principale (``src/api/assistant/``)

Cette API constitue le cÅ“ur de lâ€™assistant. Elle doit exposer plusieurs endpoints permettant dâ€™interagir avec les chaÃ®nes LangChain dÃ©finies dans ``src/core/``. Les fonctionnalitÃ©s attendues sont :

- **Analyser un code Python (``/analyze``)** : Invoque la chaÃ®ne dâ€™analyse et retourne lâ€™Ã©valuation du code.
- **GÃ©nÃ©rer un test unitaire (``/generate_test``)** : Appelle la chaÃ®ne de gÃ©nÃ©ration pour produire un test en pytest.
- **Expliquer un test (``/explain_test``)** : Utilise la chaÃ®ne dâ€™explication pour fournir une version pÃ©dagogique.
- **ExÃ©cuter le pipeline complet (``/full_pipeline``)** : Cet endpoint combine plusieurs Ã©tapes en une seule requÃªte. Le code soumis est dâ€™abord analysÃ© par la chaÃ®ne dâ€™analyse.
    - Si le rÃ©sultat de lâ€™analyse indique que le **code est non optimal**, le pipeline sâ€™arrÃªte immÃ©diatement et lâ€™API renvoie uniquement lâ€™Ã©valuation du code avec la liste des problÃ¨mes dÃ©tectÃ©s et les suggestions dâ€™amÃ©lioration.
    - En revanche, si lâ€™analyse conclut que le **code est optimal**, alors le pipeline poursuit automatiquement les Ã©tapes suivantes : gÃ©nÃ©ration dâ€™un test unitaire puis explication pÃ©dagogique du test.

- **Chat conversationnel (``/chat``)** : Permet une discussion libre avec mÃ©moire, en utilisant ``RunnableWithMessageHistory``.
- **Historique (``/history``)** : Retourne lâ€™ensemble des Ã©changes pour un utilisateur.

> âš ï¸ ***POINTS D'ATTENTION*** âš ï¸ 
>
> - Les rÃ©sultats des endpoints **``/analyze``**, **``/generate_test``**, **``/explain_test``** et **``/full_pipeline``** doivent Ãªtre **enregistrÃ©s dans la mÃ©moire associÃ©e Ã  lâ€™utilisateur**, afin que chaque interaction soit conservÃ©e dans son historique.
> - Les deux APIs doivent tourner dans **des conteneurs distincts (auth_service et main_service)**.
> - **Lâ€™API principale dÃ©pend de lâ€™API dâ€™authentification** pour vÃ©rifier lâ€™identitÃ© des utilisateurs.
> - Une gestion rigoureuse des erreurs est indispensable : toutes les exceptions doivent Ãªtre capturÃ©es et transformÃ©es en rÃ©ponses HTTP explicites.

### Suivi et Monitoring avec LangSmith 

Pour amÃ©liorer la traÃ§abilitÃ© et le suivi de lâ€™assistant, il est nÃ©cessaire dâ€™intÃ©grer LangSmith, la plateforme de monitoring et de debug pour LangChain. 

- Tracer toutes les requÃªtes envoyÃ©es au LLM, avec leur prompt et leur rÃ©ponse.
- Visualiser les chaÃ®nes et leurs Ã©tapes (prompts, parsers, mÃ©moire) dans une interface graphique.
- DÃ©boguer plus facilement en cas de problÃ¨me de format ou dâ€™erreur du modÃ¨le.
- Comparer plusieurs versions de prompts ou de chaÃ®nes afin dâ€™optimiser les performances de lâ€™assistant.

Pour activer LangSmith, vous devez configurer vos variables dâ€™environnement dans le fichier ``.env``.

### Interface Streamlit 

En plus des APIs, vous pouvez proposer une interface utilisateur dÃ©veloppÃ©e avec Streamlit. Elle rend lâ€™assistant beaucoup plus accessible et agrÃ©able Ã  tester, en offrant une interaction directe sans passer par des requÃªtes API manuelles.

**FonctionnalitÃ©s attendues**

- Authentification et Connexion
- **Analyse** : Saisir un extrait de code Python et afficher le diagnostic du LLM
- **GÃ©nÃ©ration de tests** : Fournir une fonction Python et obtenir automatiquement un test unitaire en pytest.
- **Explication de tests** : Coller un test unitaire et recevoir une explication dÃ©taillÃ©e et pÃ©dagogique.
- **Pipeline complet** : ExÃ©cuter en une seule fois lâ€™analyse â†’ gÃ©nÃ©ration â†’ explication.
- **Chat libre** : Discuter avec lâ€™assistant de maniÃ¨re naturelle, en utilisant la mÃ©moire conversationnelle.
- **Historique** : Visualiser toutes les interactions de la session en cours.

### DÃ©ploiement avec Docker et Makefile

Lâ€™ensemble du projet doit Ãªtre entiÃ¨rement conteneurisÃ© afin de garantir une mise en place simple, reproductible et indÃ©pendante de lâ€™environnement de dÃ©veloppement. 

**Services attendus**

- **auth** : lâ€™API dâ€™authentification, responsable de la gestion des utilisateurs
- **main** : lâ€™API principale, qui expose les fonctionnalitÃ©s LangChain (analyse, gÃ©nÃ©ration de tests, explication, pipeline, chat, historique).
- **streamlit** : lâ€™interface utilisateur, permettant de tester facilement lâ€™assistant via une interface graphique.

Chaque service dispose de son propre **Dockerfile** et dâ€™un fichier **requirements.txt** spÃ©cifique.

**Makefile**

Le Makefile doit centraliser toutes les commandes utiles au projet. le dÃ©ploiement complet du projet ne doit nÃ©cessiter quâ€™une seule commande :

```bash
make
```

> âš ï¸ POINTS D'ATTENTION âš ï¸
>
> - Les ports doivent Ãªtre exposÃ©s clairement et documentÃ©s dans le README.
> - Assurez-vous que toutes les variables sensibles (clÃ©s API, configuration LangSmith, etc.) soient bien stockÃ©es dans le fichier ``.env`` et chargÃ©es par **docker-compose**.

### README.md 

Votre projet doit obligatoirement contenir un fichier **README.md** clair et structurÃ©.
Ce document doit expliquer le fonctionnement global de votre assistant, ainsi que la maniÃ¨re de le dÃ©ployer et de le tester.

- Ã‰tapes pour configurer ``.env``.
- Commandes principales du Makefile (make up, make down, make logs).
- Liste des endpoints disponibles et des ports (API auth et API assistant).

**Tests**

Instructions pour vÃ©rifier que lâ€™API fonctionne correctement (scÃ©narios de test Ã  rÃ©aliser) :

- inscription
- login
- analyse
- gÃ©nÃ©ration de test
- explication
- pipeline complet
- chat avec mÃ©moire
- affichage historique


## Rappels et Conseils

Avant de commencer, gardez en tÃªte les points suivants :

- **Organisation** : Respectez scrupuleusement la structure de projet fournie. Chaque fichier a un rÃ´le prÃ©cis (LLM, prompts, parsers, mÃ©moire, API, etc.). Une bonne organisation facilitera la correction et la lecture de votre code.
- **Variables dâ€™environnement** : Ne mettez jamais vos clÃ©s en clair dans le code. Stockez-les dans le fichier ``.env``.
- **Prompts** : Assurez-vous toujours dâ€™utiliser les placeholders (**``{input}``**, **``{format_instructions}``**, etc.) et, pour le chat, le **MessagesPlaceholder** afin de bien gÃ©rer lâ€™historique des Ã©changes.
- **Parsers** : Imposez toujours un retour au format JSON. Câ€™est ce qui garantit que vos endpoints renverront des objets structurÃ©s et exploitables.
- **MÃ©moire** : Utilisez un ``session_id`` unique pour Ã©viter de mÃ©langer les historiques. Nâ€™oubliez pas que la mÃ©moire est en RAM et disparaÃ®t si vous redÃ©marrez vos services.
- Lâ€™API dâ€™authentification doit Ãªtre sÃ©parÃ©e de lâ€™API principale.
- **Docker** : Ne mettez dans vos Dockerfiles que ce qui est nÃ©cessaire. Copiez uniquement les fichiers utiles et exposez les bons ports.
- **README** : Ce fichier doit Ãªtre Ã©crit comme si un correcteur nâ€™avait aucune connaissance prÃ©alable de votre projet. Indiquez clairement comment lancer les services, et comment tester chaque endpoint.
- **Tests** : prenez le temps de tester toutes les fonctionnalitÃ©s (auth, analyse, gÃ©nÃ©ration, explication, pipeline, chat, historique). Nâ€™attendez pas la fin pour vÃ©rifier : testez au fur et Ã  mesure.

> âš ï¸ ASTUCE âš ï¸
>
> - CrÃ©ez un environnement virtuel ``uv`` avant de commencer Ã  conteneuriser votre projet.
> - DÃ¨s que vous mettez en place une nouvelle chaÃ®ne, crÃ©ez immÃ©diatement son endpoint correspondant et testez-le pour vÃ©rifier son bon fonctionnement.

## Rendus

N'oubliez pas d'uploader votre examen sous le format d'une archive zip ou tar, dans l'onglet **Mes Exams**, aprÃ¨s avoir validÃ© tous les exercices du module.

> âš ï¸ **IMPORTANT** âš ï¸ : Nâ€™envoyez pas votre environnement virtuel (par ex. .venv ou uv) dans votre rendu. En cas de non-respect de cette consigne, un **repass automatique** de lâ€™examen vous sera attribuÃ©.

FÃ©licitations, si vous avez atteint ce point, vous avez terminÃ© le module sur LangChain et LLM Experimentation ! ğŸ‰.


